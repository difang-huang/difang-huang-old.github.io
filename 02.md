---
layout: page
title: code
permalink: /code/

---

## Code

- Pearson and Spearman correlation matrix:

ssc install corsp

corsp varlist, pw sig



- String to numerical:

destring gvkey, replace

- numerical to string:

tostring gvkey, replace format(%06.0f)



- area under ROC

logistic, logit, probit, or ivprobit

lroc, nograph



- skewness



- idiosyncratic stock return volatility
  - Campbell, J. Y. and Taksler, G. B. (2003), Equity Volatility and Corporate Bond Yields. *The Journal of Finance*, 58: 2321–2350. doi:10.1046/j.1540-6261.2003.00607.x
  - Rajgopal, S. and Venkatachalam, M. (2011), Financial reporting quality and idiosyncratic return volatility. *Journal of Accounting and Economics*, 51: 1–20. doi.org/10.1016/j.jacceco.2010.06.001.

```SAS
libname local "D:\Dropbox";
 
* Remote signon and upload input database including permno and date;
%let wrds=wrds-cloud.wharton.upenn.edu 4016;
 
options comamid=TCP remote=WRDS;
signon username=_prompt_;
 
rsubmit;
 
proc upload data=local.input out=input; run;
 
* Get raw returns and FF risk factors;
proc sql;
  create table rets as
  select a.*, b.date, b.ret, c.smb, c.hml, c.umd, d.vwretd as mktret, (b.ret-d.vwretd) as exret
  from input a left join crsp.dsf b
  on a.permno=b.permno and a.enddt-180<b.date<=a.enddt
  left join ff.factors_daily c
  on b.date=c.date
  left join crsp.dsi d
  on b.date=d.date
  order by a.permno, a.enddt, b.date;
quit;
 
* Estimate factor exposures;
proc printto log=junk; run;
 
proc reg data=rets edf outest=params noprint;
  by permno enddt;
  eq0: model exret=;
  eq1: model ret=mktret;
  eq2: model ret=mktret smb hml;
  eq3: model ret=mktret smb hml umd;
run;
proc printto; run;
 
* Compute abnormal returns for all models for each trading day;
data abrets; merge rets (in=a)
  params (where=(_model_='eq0')
     keep=permno enddt _model_ _rmse_ _p_ _edf_
     rename=(_rmse_=std0 _p_=p0 _edf_=edf0))
  params (where=(_model_='eq1')
     keep=permno enddt _model_ _rmse_ intercept mktret
     rename=(_rmse_=std1 intercept=alpha1 mktret=beta1))
  params (where=(_model_='eq2')
     keep=permno enddt _model_ _rmse_ intercept mktret smb hml
     rename=(_rmse_=std2 intercept=alpha2 mktret=beta2 smb=sminb2 hml=hminl2))
  params (where=(_model_='eq3')
     keep=permno enddt _model_ _rmse_ intercept mktret smb hml umd
     rename=(_rmse_=std3 intercept=alpha3 mktret=beta3 smb=sminb3 hml=hminl3 umd=umind3));
  by permno enddt;
  var0=std0**2; var1=std1**2;var2=std2**2;var3=std3**2;
  abret0=exret;
  expret1=alpha1+beta1*mktret; abret1=ret-expret1;
  expret2=alpha2+beta2*mktret+sminb2*smb+hminl2*hml; abret2=ret-expret2;
  expret3=alpha3+beta3*mktret+sminb3*smb+hminl3*hml+umind3*umd; abret3=ret-expret3;
  nobs=p0+edf0;  /*number of observations used in estimation*/
 drop p0 edf0 std0 std1 std2 std3 _model_ exret;
 if a and nobs>=21;
run;
 
proc sort data=abrets; by permno enddt date; run;
 
proc means data=abrets noprint;
  by permno enddt;
  output out=retvol
  std(abret0)=vol0 std(abret1)=vol1 std(abret2)=vol2 std(abret3)=vol3;
run;
 
* Download output dataset and remote signoff;
proc download data=retvol out=local.retvol; run;
 
endrsubmit;
signoff;
```







# Python Code

```python
from urllib import request

from bs4 import BeautifulSoup

import re

from math import ceil

import csv

 

# Determine the number of pages to webscrape

scac = "http://securities.stanford.edu/filings.html"

page = request.urlopen(scac)

soup = BeautifulSoup(page, 'html.parser')

heading = soup.find_all('h4')[-1].get_text()

total_record_num = re.findall(r'\d+', heading)[0]

total_page_num = ceil(int(total_record_num) / 20)

 

# Webscrape all pages

container = [("filing_name", "filing_date", "district_court", "exchange", "ticker")]

i = 1

while i <= total_page_num:

    url = scac + "?page=" + repr(i)

    print(url)

    page = request.urlopen(url)

    soup = BeautifulSoup(page, 'html.parser')

    table = soup.find('table', class_ = 'table table-bordered table-striped table-hover')

    tbody = table.find('tbody')

    for row in tbody.find_all('tr'):

        columns = row.find_all('td')

        c1 = re.sub(r'[\t\n]', '', columns[0].get_text()).strip()

        c2 = re.sub(r'[\t\n]', '', columns[1].get_text()).strip()

        c3 = re.sub(r'[\t\n]', '', columns[2].get_text()).strip()

        c4 = re.sub(r'[\t\n]', '', columns[3].get_text()).strip()

        c5 = re.sub(r'[\t\n]', '', columns[4].get_text()).strip()

        container.append((c1, c2, c3, c4, c5))

    i = i + 1

 

# Write to a CSV file

with open('scac.csv', 'w', newline='') as csvfile:

    writer = csv.writer(csvfile)

    writer.writerows(container)

```

